---
layout:     post
title:      DeepDive远程监督关系抽取论文精读
date:       2018-08-14
auther:     Leslee
head-img:   img/darkness.png
catalog:    true
tags:
    - 论文精读

---


# Deepdive论文
## 摘要
DeepDive是一个从黑暗数据中提取关系数据库的系统：广泛收集和存储但无法通过标准关系工具利用的文本，表格和图像的质量。  
如果暗信息中的信息科学论文，网络分类广告，客户服务说明等等相反，在分析数据库中，它会给分析师一个庞大而有价值的新数据集。  
“与以前的信息提取系统相比，DeepDive的独特之处在于能够以合理的工程成本获得非常高的精度和召回率。应用程序，我们已经使用了DeepDiveto创建的数据库，其准确性与人类注释器相当。  
迄今为止，我们已经成功部署了DeepDivetocate以数据为中心的应用程序，用于保险，材料科学，基因组学，古生物学家，执法等.DeepDive解锁的数据代表了工业，政府和科学研究人员的巨大机会.  
DeepDiveis采用了一种不同寻常的设计，将大规模的概率推理与新的交互式交互循环相结合。这种设计是通过围绕概率训练和推理的几个核心创新实现的。  
## 介绍
DeepDive是一个从结构化数据中提取结构化数据的系统。暗数据是收集和存储的文本，表格和图像的质量，但使用传统的关系数据库工具无法获取。  
考虑一套关于保险索赔，科学图书或互联网分类广告的文字说明;这些不同的来源包含非关系格式的有价值的信息，但原则上应该适合通过关系工具进行分析。  
目前，组织支付收集和维护黑暗数据的成本，但无法利用它.DeepDive消耗黑暗数据并填充可用于标准数据管理工具的关系数据库，例如OLAP查询处理器，Tableau等可视化软件和R等分析工具。或Excel。  
黑暗数据通常包含任何其他格式无法获得的信息。例如，一家保险公司的声明类似于一个专门针对单一声明的小博客，其中的条目由许多不同的人撰写。他们将包含来自客户互动的服务代表的笔记，来自治疗索赔人的医生的账单，来自汽车维修店的报告等。简单的声明可能只有单个文本注释，但更复杂的声明可能有数百个。如果此信息在关系数据库中而不是文本中，则可以使用简单的SQL查询来回答一系列有用的问题，包括:  
- 哪些医生对索赔要求最高？
- 伤害类型的分布是否随时间而变化？
- 某些检查员是否会产生比其他人更大的索赔？  

由于分析任务的成功往往受到现有数据的限制[18]，因此深度数据中嵌入的信息可能非常有价值。在某些情况下，数据是有价值的，机构雇用人手动阅读文件并手工填充关系数据库[37]。然而，这种做法很昂贵，令人难以置信的低，并且出人意料地容易出错。  
DeepDivecan比人类更准确，更快地从暗数据中提取结构化数据库。当然，至少从20世纪90年代初开始，信息提取一直是活跃的学术研究领域[1] .Deep-Diveis因其能够生成极高的数据库而具有独特性具有合理数量的人工工程的准确性。在一系列广泛的应用中，DeepDive已经能够获得精确的数据，可以达到或超过人类注释器的数据.DeepDive的高质量是可能的，因为基于概率推理的独特设计和专业的工程开发周期;它们得到了几项技术创新的支持，可用于高效的统计培训和抽样.   
DeepDiveis是一个自2011年以来一直处于不发达状态的开源项目。我们用它来提取许多不同领域的高质量信息，包括基因组学，保险，网络分类广告，材料科学，古生物学等。我们相信，从黑暗数据中创建和开发提取的信息（现在坐在快速增长的数据湖泊中）代表了学术界，工业界和政府的巨大机会。我们已经将许多这些提取的数据集部署到特定领域的用户，并且已经被批准看到这些信息能够实现新颖和卓越的领域特定结果。  
在本文中，我们将讨论DeepDive软件架构的核心元素，以及来自学术和工业环境的DeDDive驱动的一些应用程序。DepreDive背后的许多算法和技术组件都是以前记录过的;本文是第一个全面展望软件，将技术元素联系在一起的整体。尽管DeepDive设计用于处理多种媒体类型，但我们主要关注文本。  
本文主要内容有：
- DeepDive的设计目标。（第二章） 
- DeepDive的软件架构。 （第三章）  
- 简要概述实现上述架构所需的技术组件  （第四章）
- DeepDive优化调参过程。（第五章）
- 案例解析 （第六章）
## 用户交互和设计标准   
DeepDive的首要目标是以非常高的质量提取数据。图1显示了DeepDive的核心输入和输出。用户通过输入文档信息呈现DeepDive，并从该语料库中输入信息填充信息。例如，如果文献资料是基因组学研究论文的集合，那么令人兴奋的模式可能是（基因，表型）.DeepDivethen发出一个具有分析的输出期望的平板电脑并使用在输入语料库中找到的信息。  
DeepDive的抽取是否成功依赖于输出期望表的准确率和召回率，DeepDiveobtains高质量的要求工程师通过提供域特定代码重复检查输出表，诊断问题和修复问题。DeepDive通过要求工程师通过提供特定于域的代码重复检查输出表，诊断问题和修复问题来获得高质量。 通过反复进行这个迭代改进循环，一个DeepDive工程师应该能够获得极高精度的数据。  
DeepDive的成功完全取决于这个迭代开发周期的有效和快速执行。在本节中，我们将讨论此开发人员模型背后的动机，以及由模型驱动的系统设计标准。  
### 迭代开发模型  <useless>
我们观察到，在处理信息提取任务时，很容易花费时间。即使是训练有素的计算机科学家也会以临时的方式工作并贡献无用的功能。非常费力的完美功能，影响很小。解决明显而非常见的错误案例，愚弄统计基础设施，很少知道何时添加更多的培训数据，以及降低受害者共同（尽管有时是微妙的）统计错误。简而言之，即使我们中最好的人也面临着低精度或召回数量的目标 。。  
这个过程非常类似于性能调试--软件工程师很容易花费大量的时间来处理问题，这使得变更无法产生具体的结果，但与提取器开发相比，想要快速运行慢速软件的软件工程师可以采用几种非常有效的方法，如果不是总是实践的话。   
一位有效的工程师知道阿姆达尔定律中的智慧，并且只关注最慢的成分，识别最小的组件通常是违反直觉的，因此在进行任何更改之前，她会对代码进行严格的检测。最后，她知道她应该进行性能改进，因为解决问题可以改变组件现在成为瓶颈的整体情况。通过重复应用这种根深蒂固，可靠和可预测的学科，工程师可以获得快速的软件。  
**DeepDive旨在使用类似的开发周期来获得高精度。人工工程师识别输出表中最常见的错误。 然后，检查原始输入语料库的相关错误语料，以及DeepDive的仪表提取决策。 此信息使工程师能够解决错误类，重新运行系统，然后继续执行下一个错误。** 我们将在第5节中更详细地描述开发人员的体验
## 系统设计标准  
三个核心设计标准：
1. 机制独立性 -- 开发人员通过提供领域信息来提高准确性，而不是通过更改统计过程。
2. 集中加工 -- 提取，清洗和整合的相互关联的步骤应该同时处理。
3. 可调试性 -- 可调试的决定|由系统产生的任何提取错误都应该是可分析的并且与根本原因相关联，因为可以被人工工程师理解和定位到。  
## 机制独立性
**开始批判**
```
为了获得高精度或更快的运行时间，机器学习工程师通常会对给定任务的核心统计过程进行修改。  
机器学习会议主要侧重于新颖的方法，经常通过实验证明可以提高任务工作量的准确性。  
从工程的角度来看，这种做法是灾难性的，鼓励开发人员最容易出错并且难以对系统进行调试。  
它还会分散开发人员的注意力，使其无法贡献人类独有的贡献,  
**通常解决真正的提取问题的方法：额外的问题 - 领域知识**
```    
**与机器学习工具包（如MLib，MADLib或SAS）不同，DeepDive并不是用户精心设计的统计算法包。其训练和推理过程不受到开发人员控制。相反，开发人员使用名为DDlog的高级数据目录语言(datalog-like language)来描述结构化提取问题。** 然后DeepDive开始发现如何最有效的解决开发人员描述的抽取任务。对于数据库社区中的读者来说，这种方法听起来非常熟悉，长期以来一直倾向于高级语言而不是直接用户控制查询处理机制。然而，这种设计选择在20世纪80年代提出了严重的技术挑战，直到最终知道如何建立一个可以获得可接受性能的查询处理系统。而DeepDive面临的问题是：1.怎么开发一款非常有效的用于概率推理的系统？解决这一挑战是构建DeDDive的关键障碍（尽管DeepDive不是使用这些高效训练和推理技术的唯一可能方式）。  
## 数据集中处理  
在其他的提取处理流程中，往往将数据清洗，提取，集成和清理是分开的。在DeepDive中将他们集成在一起，作者认为集成处理对支持开发人员工作流程至关重要。关键是集成处理允许开发人员解决最容易解决的特征提取问题。   
例如：DeepDive的开发人员想从书评网页中创建图书目录，使用两个单独的步骤进行：首先提取，然后与部分目录集成。考虑目标schema（模式）是：(booktitle,author,uniqueID,price)并且提取器有很高的准确率，达到98%。剩下的2%是抽取到的不是书籍，由于NLP解析器出错，导致错误提取的电影名字。。这样做，导致下游的集成模块没有办法集成一些正确的提取，和所有的不正确的提取（因为现有的数据库不包含电影）。  
在孤立的数据处理系统中，提取器提供了不太正确的数据，所以导致失败。此时集成系统的team会告诉提取team数据出现无法集成的错误。不幸的是，NLP解析问题很可能让提取团队难以修复，迫使他们投入巨大的技术资源来解决问题，或者投入人力资源来手动审查提取器的输出  
··················以上基本都是废话·························  
对于集成团队来说，完全删除包含电影标题的提取元组（其中有免费和高质量的可下载数据库）将会非常简单。 但是，这种简单的修复方法在孤立系统中并不明显; 对于负责集成端到端数据质量的工程师来说，这是显而易见的。  
···这句话的意思是：NLP带来的问题，NLP不能解决，但是在集成过程中别的流程有办法可寻。···  
说孤立的提取器不能使用字典似乎是假的; 我们完全同意。对于由不同团队编写和维护的每个阶段使用单独的软件组件将不可避免地导致这些人为的区别，这将导致糟糕的工程决策。在集成系统中，开发人员只需询问最终数据产品是否足够好;如果答案是否定的，开发人员会选择最方便的地方来解决最重要的问题。  
在联合参考任务期间，在合并多个信息源方面还可能存在进一步的统计优势。  
## 调试决策  
最终设计目标是DeepDive应该可以做出可靠的决策，当开发人员识别出提取错误的时候，必须可以检查到DeepDive做出错误决策的原因，满足这个目标对系统有三个实际的影响。  
+ DeepDive使用传统Python代码实现的人类可理解的功能，而不是深度学习系统产生的不透明功能。在第5节中，我们描述了如何使用这种方法，并且仍然获得了与深度学习相关的一些特征诱导优势。  
+ 使用校准概率而不是分数来描述加权决策。这种做法允许人们检查个人和中间决策，然后决定这个中间决策是正确还是错误。除非是数值型的人类可理解的东西，要不然是做不到这一点的，所以这对概率推理机制也有一定的影响。  
+ 保留了统计中的“执行历史”并且可以很方面供用户调阅，例如，我们的调试工具始终为每个特征显示在训练数据中观察到特征的次数。这使得工程师可以检测到由于训练数据不足而导致该特征的权重不正确。  
# 系统概述  
图2 显示出DeepDive的三个执行阶段。  （DeepDive作为输入暗数据，输出结构化数据库。
为了提高质量，开发人员添加了新的规则和新数据，并对系统当前快照的结果进行了错误分析.DeepDive提供了一种声明性语言来指定不同规则和数据的每种类型，以及逐步执行此迭代过程的技术。）
![image](https://note.youdao.com/yws/public/resource/831d6e08c83aac655a96eca3441a6b1d/xmlnote/982B9CACCFD24126BCBF1806EB935397/5324)  
1. 首先生成候选，DeepDive将使用用户自定义函数(UDF)应用于输入语料中的每个文档，产生候选提取；例如，专门提取书的价格的提取器可以从每个输入网页传出数值。  候选生成器的目标是消除明显的错误的输出，（例如价格提取器中不是数字的值），所以，候选生成的主要目标是：高召回率，低准确率。 该系统还未每个候选应用用户自定义的特征函数。 例如，可以在提取过程中编写价格候选左侧是否有￥表示金钱的符号等。 我们认为，提高特征质量是统计系统提高其质量的核心机制之一。这一点跟深度学习的合成特征不太类似，该作者坚持的是人类应该理解所有的特征，以便接下来的调试。
2. 在监督过程中，DeepDive使用用户自定义远程监督规则为某些候选提供标签，例如，我们之前已经有了一个手动标注数据库，这样我们可以知道下载网页子集的真实价格。监督规则的原则应该是：高精度，低召回率。虽然远程监督规则在某些情况下标记数据会比较容易出错，但是它允许我们执行规则进行标注，并且可以生成大量标签示例。  
3. 在学习和推理过程中，我们构建了一个图模型，表示所有的标签化的候选提取、训练权重、然后推断每个候选的正确概率。在这个过程结束后，我们对每个推理得到概率取阈值，从而获得输出数据库的正确提及。  
每一次用户通过图1的方式进行迭代循环的时候，每次更改候选生成的UDF，更改特征UDF，或者更改远程监督规则的时候，都会重新运行DeepDive从而获取到新的输出表。  
用户还可以指定很多小的任务特定的细节，比如：查找输入数据，改变输出模式，合并多个相关的可能的提取信息。这些都通过DDlog来实现。  
## 候选生成和特征提取
所有的DeepDive中的数据都存储在关系数据库中，第一步使用一组sql查询和用户自定义函数填充数据库。一般情况下，DeepDive存储数据库中所有文档的句子级别。然后使用标准的NLP预处理工具生成标记，包括去除HTML标签，词性标注，实体识别和句法分析。经过这两个步骤以后，DeepDive开始执行比较关键的两个步骤：(1)候选映射。<哪些是产生可能的提及、实体与关系的SQL查询>。(2)与候选相关的特征提取器。  

```
例3.1 候选映射通常都比较简单，这里我们为一个句子中的实体对创建一个关系提及。  
MarriedCandidate(m1;m2) : -PersonCandidate(s;m1);PersonCandidate(s;m2):
```  
候选映射只是使用UDF的SQL查询，该查询只提供高召回低精度的ETL脚本，这些规则必须是高召回率的，如果候选映射单元错过任何一个事实，DeepDive就再也没有机会提取到它。  
我们同样需要特征提取，这里我们从两个方面扩展经典马尔科夫逻辑：(1)用户自定义函数和(2)权重绑定。具体示例如下：  
```
ex3.2 假设一个UDF phrase(m1,m2,sent)返回一个句子中两个实体提及之间的短语。   
例如，配偶关系中的“and his wife”，这个短语可能表明两个人是否是配偶关系。
MarriedMentions(m1;m2) : -MarriedCandidate(m1;m2),Mention(s;m1),  
Mention(s;m2),Sentence(s;sent),weight=phrase(m1;m2;sent)  
这个可以考虑为分类器问题：这个规则说明一点--一段文本是否表明提及m1和m2之间  
存在婚姻关系，是收到两个提及之间的短语的影响的。  
DeepDive将根据基于训练数据的推断，通过估算权重的方式，计算两个提及是确实结了婚的。
```    
···················
技术上讲，针对一个句子中给定的关系提及，phrase会返回一个标识符用来确定应该使用那些权重???<这个权重weight=phrase(m1,m2,sent),是不是说，m1，和m2，和sent之间使用的特征??>
···················这一点有待进一步思考了解。  
如果phrase在两个关系提及中返回相同的结果，他们将会收到相同的权重，通常，phrase可以是以元组方式操作的任意UDF。。这个将会允许DeepDive支持很多通用的特征示例，比如：“bag of word”(词袋模型)、具有上下文感知的NLP特征以及特定域的词典和本体。除了指定分类集外，deepdive继承了马尔科夫逻辑通过加权规则指定实体之间存在丰富关联的能力，这些规则对数据清洗和集成非常有用。
## 监督  
和马尔科夫逻辑一样，deepdive可以使用训练数据或者任何关系的证据，特别的，每一个user relation 都跟一个证据变量（evidence relation）相关联，并且证据变量还附加一个字段，指示该条目是真还是假。继续我们的例子，证变量脸关系，MarryMention_Ev 应该包含 含有正面和负面标签的提及对。作为一个规则，我们使用远程监督进行标签标注取代手动标注。  
```
ex3.3 远程监督是在信息抽取系统中很流行的创建证据(标签)的技术。  
这个idea的想法是使用一个不完整的KB对数据库进行启发式标注作为  
正确标签，从而链接到一堆已婚实体的所有的关系提及中去。
MarriedMentions_Ev(m1, m2, true) : -
MarriedCandidates(m1, m2), EL(m1, e1),
EL(m2, e2), Married(e1, e2).
```
这里Married是一个不完整的真实世界中人物的结婚名单，并且我们希望扩展它。关系RL是指实体链接，将实体提及连接到其候选实体上。远程监督的方法虽然存在一定的噪声。确实这个技术会产生一定的错误的标记的配偶对。机器学习技术可以通过冗余来应付这些噪音，并且学习到相关的phrase(例如，and his wife)。负例则是很大程度上不相关的关系产生的(例如兄弟姐妹)。远程监督利用模式和关系实例之间的“二元性”，此外，它还允许我们将整个想法整合到deepdive的统一概率框架中。  
## 学习和推理  
在学习和推理阶段，DeepDive生成一个因子图，非常类似于马尔科夫逻辑。<因子图解释：https://blog.csdn.net/sysstc/article/details/76590278 ， https://blog.csdn.net/sysstc/article/details/77067537>  并且使用标准吉布斯采样技术完成推理和学习，下面会详细介绍。  
在图4中，DeepDive使用一组SQL查询显式的构造用于推理和学习的因子图。现在表示一个因子图(一个元组):(v，F，ω)，其中v表示一个对应于布尔随机变量的节点集合。F是一组超边集合，ω表示权重函数，DeepDive中，每个变量对应于数据库中的一个元组，每个超边对应于规则γ的grounding集合。（grounding：Grounding：在predicate中，将所有可能的常量替换掉所有的变量。也就是将所有的变量抽象成函数的形式叫做grounding，比方 a has wife b 抽象成has wife(a,b)和has wife(c,d)）.在DeepDive中，V和F使用一组SQL进行显式的创建，然后将这些数据结构传递给数据库外运行的采样器，用来估计数据库中每个节点或者元组的边际概率。
图4 grounding示意图，每个元组都与因子图中的布尔随机变量和节点想对应，为每组grounding创建一个因子。
![image](https://note.youdao.com/yws/public/resource/831d6e08c83aac655a96eca3441a6b1d/xmlnote/F075459B2C1F4F1E83C670237FF3C3D2/5636)
```
以图4中的数据库实例与规则为例，关系R，S和Q的每个  
元组都是一个随机变量，V包含所有的随机变量。  
推理规则F1和F2因子在因子图中具有相同的名称，且F1和F2都是用SQL来实现
```  
为了确定语义，我们首先定义“可能的世界(possible world)”概念：它是一个每个变量赋值为真值的函数。现在令“Ι”表示所有的可能的世界的集合，对每个因子f和一个可能的世界I，我们使用ω(f,I)来表示一个因子f成为可能的世界的返回值。(大概可以理解为一个因子，最后取真值的可能性的参数)。然后我们定义W(f,I)=∑ω(f,I)。然后，可能世界的边际概率如下函数：  
![image](https://note.youdao.com/yws/public/resource/831d6e08c83aac655a96eca3441a6b1d/xmlnote/C7AC07421B6B4026A45F13756AF8C45D/5684)  
类似于归一化过程。。
Deepdive在因子图上的主要任务就是统计推断，例如，对于一个给定的节点，这个节点的可能世界取值为1的边际概率是多少？这个边际概率的定义如下:给定一个变量v，令
I+为被判别为真的所有真实世界的集合。则v的边际概率就是∑Pr(I+).  
## 执行
DeepDive按照顺序执行上述三个阶段，并且在学习和推理阶段结束，为每个候选事实获得边际概率。为了输出最终的数据库，Deepdive通过允许用户自定义概率阈值，例如p>0.95,对于牺牲精度高recall的，可以使用更低点的阈值。  
通常，开发者还需要曝光错误分析，并且重复开发过程。错误分析是理解常见错误的过程(错误的候选提起，错误的特征，错误的提取)。为了方便错误分析，用户可以编写标准sql查询，或者使用工具。在第五节会详细阐述。  
# 系统基础
DeepDive依赖于两个先进的技术：增量grounding和统计推断与学习。  
## 增量grounding  
grounding发生在当deepdive将关系和规则的集合转换成可能的概率推理的具体的因子图，因为DeepDive是基于SQL的，我们可以利用多年的增量视图技术维护工作。这个阶段的输入与Grounding阶段的输入是相同的，一组SQL查询和用户schema。这个阶段的输出是grounding怎么变化。例如，一组修改后的变量ΔV和他们的因子ΔF，由于V和F都是数据库中的视图，因此任何视图技术都可以应用于grounding 。deepdive使用DRed算法(逐步维护视图技术).来处理添加和删除。在DRed中，在用户schema中的每个关系Ri，创建一个ΔRi，这个东西跟用户schema具有相同的结构，只是增加一列count。对每个元组t，t.count表示Ri中的t的导数。一次更新的时候，Deepdive分两步更新delta关系，首先，在对增量delta Ri ，deepdive更新相应的计数。第二，执行成为增量规则的SQL查询，该查询处理这些计数后以生成修改后的变量V和因子F。DRed的开销是适度的，因此除了初步加载外，总是运行DRed。  
## 统计推断与学习  
deepdive的主要任务是统计与推断，给定一个node，这个节点的值取1的概率为多少？由于当前元组在输出中节点取值为1，因此该过程计算返回给用户的边际概率。通常，计算这些边际概率的是#P-hard。DeepDive通过使用吉布斯采样预测每个元组的边际概率。  
1.效率和可扩展性  
    统计算法由两部分组成：统计效率，一个算法的大致收敛步数；二是硬件效率，每个步骤的效率如何。  
DimmWitted。怎么在一个机器上设计高性能的推断与学习的引擎是研究重点。DimmWitted模型吉布斯采样作为列到行操作，每行对应于一个因子，每列对应一个变量，矩阵中的非零元素对应于因子图中的边。  
要处理一个变量，DimmWittd将获取矩阵的一列以获取一组因子，而其他列则获取到链接到同一个因子的变量集合，这个东西快，比传统的快。。。  
关于效率部分暂且不表。。。。  
2.增量推理  
由于我们选择了增量grounding，因此对Deepdive的推理阶段的输入是一个因子图和一组变化的变量和因子。目标是计算系统算出来输出概率，我们的方法是将增量维护问题构建为近似推断。从嘈杂数据中提取数据库时，程序和数据会经常发生变化，Deepdive可以同时应对两种变化。  
有两种流行的近似推理技术：基于抽样的物化（受基于抽样的概率数据库，如MCDB [22]的启发）和基于变异的物化（受近似图形模型技术的启发[49]），我们对各种DeepDive程序进行了这两种方法的实验评估。 我们发现这两种方法对因子图的大小变化，相关性的稀疏性以及未来变化的预期数量都很敏感。在空间的不同点上，性能变化高达两个数量级。 要自动选择实现策略，我们使用简单的基于规则的优化器  
# 开发和Debugging  
Deepdive的开发标准应该是远远超过同类其他关系抽取软件，但是到目前为止只是讨论了其三个特性，Deepdive将这些标准框架抽取，转化为单个集成的概率推理任务，以及可以快速执行概率推理的基础架构，无需任何人对概率推理进行操作，另外生成一堆类似于日志的代码。。  这里讨论怎么拿到高质量的输出。  
## 改进迭代循环  
通过改进迭代过程可以实现高精度，开发迭代过程的来自于传统的工程debug行为，慢慢找到系统中所有的错误加以修复最终得到完整的系统。想完整的走完这个流程，需要大量的训练人力，等。。  
这里在说机器学习驱动的数据工作并没有得到更高更好的精度，相反，随着时间推移，总是通过修补统计机制和增加特征的方式会导致生产率低下。  
下面介绍一些常见的识别纠正错误的方法，并且遇到的一些故障的解决办法。。。  
## 修复失败的情况  
快速识别和修复精度误差是DeepDive获得高精度的核心机制。此过程的第一步是工程师进行错误分析。这是一个强大的程式化文档，帮助工程师确定：  
1. 提取器的真正精确度和召回率  
2. 观察到的提取器故障模式的枚举，以及每种故障模式的错误计数。例如，医生的一个提取器可能会错误地产生城市名称，因为它们出现在Dr.之后。
3. 对于出现的排名比较靠前的错误，应该是deepdive的本身出现了错误。 

错误分析文档的作用类似于软件工程师的性能检测工具。不幸的是，与大多数性能工具不同，产生错误分析不能完全自动化，它需要人来参与。  
1. 手动标记提取样本，通常大约100--可以是正例或者负例，此示例将用于计算DeepDive的精度。  
2. 从文档中手动标记已经提取到正确答案的样本。通常大约100--正确提取了或者未正确提取。此示例将用于计算DeepDive的召回率。  
3. 将几个错误提取的样例分别归类到不同的“失败模式类”里面去，这个类不是严格定义的，而是类似于工程师看到相似错误的错误类型标签。说白了，就是差不多的错误归到同一类看是否能形成是哪里的错误导致这样的原因！  
生成错误分析需要人工注释并且会耗时，错误分析文档还包括用户无需手动生成的信息：描述输入和输出的商品统计信息，所有数据产品和代码的校验和，以及功能摘要，包括其获得的权重和观察计数。它还有一个版本检查和对包含相关功能和远程监控代码的GitHub版本的引用。  

在得到错误分析后，通常对故障模式进行排序，然后去解决错误最多的类型。  
1. 候选生成器无法识别提取候选，或者预处理错误导致没有发现候选，这一点很容易检查出来。  
2. 没有可以成功区分正确答案的特征函数。工程师通过检查当前存储桶中的每个不正确的示例来检查此故障，并询问观察到的特征是否足以让人类区分正确答案和错误答案。例如判断医生名称的时候只有一个特征 即该Dr.是不是在名字左侧，这样的话就不行，因为特征太少了。  
3. 如果前两项都是正确的，那么就是该软件在学习和推理的时候生成了一个非法的结果，然后学习到的特征权重也是不对的，**最常见的是远程监督规则不完善**，当远程监督规则为某些特征提供了很少的标签的时候，这些就容易发生。例如，Dr是否出现在候选的左侧，或者说候选是否在包含在地址里面，则远程监督规则必须为候选的两种特征提供标签。如果远程监督规则恰好标记了非地址标签，那么软件就没有足够的证据来正确计算这些特征的相对权重。应该添加新的远程监督规则来覆盖新的案例。  

以上这些过程迭代次数越多，DeepDive的数据质量就越高。当训练跑完以后，会有一系列绘图给出，这个数据帮助开发者理解系统概率输出而不必了解内部的统计算法，例如：
![图5](https://note.youdao.com/yws/public/resource/831d6e08c83aac655a96eca3441a6b1d/xmlnote/D9E37E14E13441CEB085174B7F18098E/5992)  
其中最左边的表示deepdive的发出概率是否是正确的，对于评估的概率为20%的所有项目，是否意味着只有20%是正确的提取呢？这里红色的线并没有覆盖蓝色的线，这表明缺乏深度有效的特征去计算完全正确的概率。中间和右侧的分别表示测试和整个数据集中的各种概率值所包含的样本数。理想的直方图应该是U型的，大多数项目为0或者100%，这里的直方图并不是，这表明对于很多测试例子，该软件没有足够的概率将其归到1或0.  
## 设计选择  
**特征一定是人可以理解的*||特征一定是人类可以理解的，并且是可以修改的。注意，人类可理解性是上述调试循环的核心。在我们看来，人类工程师必须能够确定观察到的错误是由于缺乏有效特征还是由于其他一些失败原因造成的; 如果特征集不可理解，则无法确定。
引入了一个特征lib的系统，可以自动的进行特征提取，这些特征可以在很多领域合理的工作，然后使用统计正则化来抛弃除了最有效的特征外的其他特征。这个东西有点深度学习的味道。但是我们的特征是可理解的。。  
**远程监督**||我们的标准开发迭代每次都会要求手动标注训练数据，现在的系统是只有在要求测试和错误分析的时候才标注数据。使用distant supervision的三个原因，1.因为比手动标注繁琐程度相比是可接受的。2.远程标注可以标注大量的标签，比手动标注的数据多。3.远程监督可以修改，增删规则，方便重新执行。  
**确定性的规则** || 这个最好的解释是：应该最大程度的避免工程失败。当面对提取任务的时候，如果可以通过正则表达式获得中等质量的数据，这样的做法是可以生产比较好的数据。并且可维护。但是正则表达式并非万能，一般第一个最有用，第二个第三个第四个就基本没用了。。  
# 应用案例  
## 医学遗传学  (翻译)
生命科学领域的文学作品多年来一直在快速增长，以至于科学家仅凭阅读和记忆（甚至在关键词搜索的帮助下）进行研究已变得不切实际。因此，有许多倡议从研究文献中构建数据库。例如，OMIM是人类基因和遗传疾病的权威数据库。它可以追溯到20世纪60年代，并且含有大约6,000种遗传性疾病或表型。由于OMIM是由人类策划的，它多年来以每月大约50个记录的速度增长。与斯坦福大学的Gill Bejerano教授合作，我们正在开发医学遗传领域的深度应用。具体而言，我们使用DeepDive从文献中提取基因，疾病和表型的提及，并统计推断它们之间的关系。
考虑一个非常简单的调节关系模式，包括（基因，表型，研究论文）。该数据库对于治疗遗传性疾病至关重要：考虑到医生认为可能与遗传相关的症状的患者。医生采取两个步骤来寻找可能的临床干预：（1）她对患者进行了测试，以确定哪些基因是非典型的，（2）她检查科学文献，看看患者的症状与患者之间是否存在任何已知的联系。非典型基因。第二步|有时候非正式地称为“医生谷歌”可能耗费巨大时间：科学文献广泛，基因和表型通常以不同的名称所知，并且识别基因和表型之间的相关关系需要阅读和理解论文医生可能花费大量时间研究罕见的遗传物质，如果谷歌搜索没有立即产生结果，那么医生很难区分人类缺乏对某个主题的临床知识与特别具有挑战性的互联网浏览能力。结果，医生花费大量时间进行研究而不是治疗患者，并且在某些情况下可能无法确定对于制定治疗至关重要的文献，如果医生可以访问（基因，表型，研究论文）数据库的高质量版本，那么查找患者信息将非常简单快捷.DeepDive可以很好的进行数据库的构建。
## 药物基因组学  
了解体内化学物质的相互作用是药物发现的关键。然而，这些数据中的大部分都存在于生物医学文献中，并且不容易被接受。药物基因组学知识库是一个高质量的数据库，旨在诠释文献中药物，基因，疾病，遗传变异和途径之间的关系。随着文献的指数增长，人工管理需要对特定药物或基因进行优先排序，以便与当前的研究保持同步。与斯坦福大学的Emily Mallory和Russ Altman [30]教授合作，我们正在开发药物基因组学领域的深度应用。具体而言，我们使用DepDive提取基因，疾病和药物之间的关系，以预测新的药理学关系。
## 反人口贩卖
人口贩运是一种可恶的罪行，它使用物质，经济或其他胁迫手段从人类那里获得劳动力。交通工具经常用于性工作或工厂工作。不幸的是，在美国识别人类贩运受害者通常会使当局感到困难个人可能不会与朋友或家人保持经常联系，也可能无法，害怕或羞于自己联系执法部门。（从一个国家到另一个国家的交易模式。）如果执法可以做得更好主动识别和调查可能的交通受害者。有点令人惊讶的是，网上有数据可能包含跟踪活动的证据。像许多其他形式的商业一样，性工作广告现在在线;有许多网站提供性服务提供商发布包含价格，位置，联系信息，物理特征和其他数据的广告。跨网站的详细信息，但在大多数情况下，帖子类似于Craigslist风格的文本分类广告：它们具有非常少的结构，许多极不标准的英语，并且往往包含相同的数据值。很容易想象一个关系模式，它可以捕获这些广告中的大部分相关数据.在讨论论坛网站上提供了更多信息，其中性服务用户发布关于他们经历的信息。同样，这些帖子主要是文本的，但往往包含类似的信息：当发生的时候，发生了什么，性工作者的细节等等。对于相当大比例的论坛帖子，我们有足够的联系信息来识别相关的性工作者广告。这些帖子也非常坦率，描述了非法活动本身以及在原始广告中从未发现过的有关性工作者的详细信息（例如药物滥用或身体虐待的证据）。对我们来说，最初令人惊讶的是，论坛作者都愿意这么做。论坛帖子似乎有一个强烈的社会因素，大致类似于什么可以在亚马逊和Yelp讨论论坛中观察到;性交易中的领域专家向我们报告说，这对他们来说并不奇怪。  
我们运行了DeDDiveon大约45M广告和0.5M论坛帖子，创建了两个截然不同的结构化表格。我们可以用很多不同的方式使用这些数据  
1. 来自广告的统计数据可以被他们自己用来识别交通警告标志。例如，从相对快速连续的多个城市发布的性工作者可能会被一个地方移动到另一个地方，以便保持迷失方向和易于管理。发布价格异常低的工人愿意从事极其危险的行为，也可能是一个交易受害者。我们为其他公司提供了提取的数据，以便使用执法产品;这些数据已经部署到几个执法机构，并且已被用于至少一次逮捕。
2. 仅使用来自广告的价格数据，我们就可以计算出美国的有关商业交易的汇总统计数据和分析。我们正在与经济学家和政治科学家一起撰写关于个体城市性经济定价与这些地区合法经济之间关系的论文。
3. 使用联合广告和论坛表格，我们可以使用论坛海报关于经验的评论来识别可能处于危险中的性工作者。例如，药物或身体虐待的迹象本身就令人担忧，也可能是可能的交易的证据。Lattice Data（http://lattice.io）的工程师，一家设计了设计使用DeepDive的初创公司，已经使用这些数据为执法人员创建面向用户的分面搜索工具。  
# 可能存在的问题  
远程监督规则是用于生成标记数据的算法方法;它们可能不如传统的手工标记数据样本准确，但是当应用于大型语料库时，它们可以生成比手标记更多的标记数据。不幸的是，如果远程监管规则与特征功能非常相似，那么标准的统计训练程序将会无法训练得到可用的Model。即使用户提供了大量有用的Feature，也不能得到一个model完美地预测标记的答案，因此训练过程将构建一个模型，将所有权重放在与监督规则重叠的单个特征上。训练统计模型将足够合理现实世界中的有效性，其中特征与事实并不完全重叠。这种故障模式极难检测：对于用户来说，只是看起来训练程序失败了。更糟糕的是，虽然用户提供了所有的有用的信息，但是却无法得到想要的输出结果，而且全程用户什么都没做错。






